---
title: "Project 2 Final"
author: "Siyeon Shaun Kim (sk5ps), Alex Stern (acs4wq), Josh Eiland (jhe5ah)"
date: "11/26/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(lubridate)
library(glmnet)
library(ROCR)
attach(train_prepared)
library(tree)
library(randomForest) ##for random forests (and bagging)
library(gbm) ##for boosting

```

## Data Cleaning and Feature Engineering
```{r}
# importing the train and test data sets
train <- read.csv("sales_manhattan_train_set.csv")
test <- read.csv("sales_manhattan_test_set.csv")
# we must coerce the data into more purposeful data types
# high end, tax class, and zip code should be categorical
train$high.end <- as.factor(train$high.end)
train$TAX.CLASS.AT.TIME.OF.SALE <- as.factor(train$TAX.CLASS.AT.TIME.OF.SALE)
train$ZIP.CODE <- as.factor(train$ZIP.CODE)
# and square footages should be numeric
train$LAND.SQUARE.FEET <- as.numeric(train$LAND.SQUARE.FEET)
train$GROSS.SQUARE.FEET <- as.numeric(train$GROSS.SQUARE.FEET)
# missing data in YEAR.BUILT must be accounted for
# binary T/F variables are created if the building had any commerical/residential units
levels(train$BUILDING.CLASS.AT.TIME.OF.SALE) <- levels(train$BUILDING.CLASS.AT.PRESENT)
train <- train %>% mutate(
  COMMERCIAL.UNITS = ifelse(COMMERCIAL.UNITS==0,F,T),
  RESIDENTIAL.UNITS = ifelse(RESIDENTIAL.UNITS==0,F,T),
  TOTAL.UNITS = ifelse(TOTAL.UNITS==0,F,T),
  # assuming that buildings missing years were constructed in 1932
  # since this is when our records begin
  YEAR.BUILT=ifelse(YEAR.BUILT==0,1932,YEAR.BUILT)
)
# We also attempted to look into whether the building class had changed and whether this was a relevant factor using the following code:
# BUILDING.CLASS.CHANGE = ifelse(BUILDING.CLASS.AT.PRESENT!=BUILDING.CLASS.AT.TIME.OF.SALE,T,F),
# but this revealed that in fact the building class had changed for every single record in the dataset, meaning that it wasn't a helpful engineered feature

# Lubridate was used to extract the month, day, and year of the sale
train$sale_year <- year(mdy(train$SALE.DATE))
train$sale_month <- as.factor(month(mdy(train$SALE.DATE)))
train$sale_day <- as.factor(day(mdy(train$SALE.DATE)))
train$sale_dayofweek <- as.factor(wday(mdy(train$SALE.DATE)))

# determining old the building is at the time of sale?
train$sale_minus_built <- (train$sale_year - train$YEAR.BUILT)
# stripping the house numbers and apartment numbers from addresses to get just the street names
train$ADDRESS <- as.factor(gsub(",.*$", "", sub("^\\S+\\s+", '', train$ADDRESS)))
# drop features that will not be used
train_prepared <- train %>% dplyr::select(
  -BOROUGH, -EASE.MENT, -APARTMENT.NUMBER
)

summary(train_prepared)
```
We got rid of BOROUGH, EASE.MENT, and APARTMENT.NUMBER because the BOROUGH variable has no variation in values (always 1) and all the EASE.MENT values are NA, while the apartment number (though potentially useful with enough advanced parcing) was too complex and variable for us to break down. We stripped the details of the addresses to include just the street names so that it could potentially be used to help predict values of future sales on the same street. We also factorized high.end, TAX.CLASS.AT.TIME.OF.SALE, ZIP.CODE, RESIDENTIAL UNITS, COMMERCIAL UNITS, sale_month, sale_day, and sale_day_of_week. Below, we implemented the same data process to the test training set.

```{r}
# we must coerce the data into more purposeful data types
# high end, tax class, and building classes should be categorical
test$TAX.CLASS.AT.TIME.OF.SALE <- as.factor(test$TAX.CLASS.AT.TIME.OF.SALE)
test$ZIP.CODE <- as.factor(test$ZIP.CODE)
# and square footages should be numeric
test$LAND.SQUARE.FEET <- as.numeric(test$LAND.SQUARE.FEET)
test$GROSS.SQUARE.FEET <- as.numeric(test$GROSS.SQUARE.FEET)
# missing data in YEAR.BUILT must be accounted for
# binary T/F variables are created if the building had any commerical/residential units
test <- test %>% mutate(
  COMMERCIAL.UNITS=ifelse(COMMERCIAL.UNITS==0,F,T),
  RESIDENTIAL.UNITS=ifelse(RESIDENTIAL.UNITS==0,F,T),
  TOTAL.UNITS=ifelse(TOTAL.UNITS==0,F,T),
  # assuming that buildings missing years were constructed in 1932
  # since this is when our records begin
  YEAR.BUILT=ifelse(YEAR.BUILT==0,1932,YEAR.BUILT)
)
# lubridate used to extract the month, day, and year of the sale
test$sale_year <- year(mdy(test$SALE.DATE))
test$sale_month <- as.factor(month(mdy(test$SALE.DATE)))
test$sale_day <- as.factor(day(mdy(test$SALE.DATE)))
test$sale_dayofweek <- as.factor(wday(mdy(test$SALE.DATE)))
# how old is the building at the time of sale?
test$sale_minus_built <- (test$sale_year - test$YEAR.BUILT)
# stripping the house numbers and apartment numbers from addresses to get just the street names
test$ADDRESS <- as.factor(gsub(",.*$", "", sub("^\\S+\\s+", '', test$ADDRESS)))
# drop features that will not be used
test_prepared <- test %>% dplyr::select(
  -BOROUGH, -EASE.MENT, -APARTMENT.NUMBER
)

summary(test_prepared)
```


## Exploratory Data Analysis 
```{r}
# A plot showing high end vs. non high end sales by block and gross square footage
ggplot(data=train_prepared)+geom_point(mapping =aes(x=BLOCK, 
                                                       y=GROSS.SQUARE.FEET,color=high.end))
# It reveals little clear correlation other than an apparent lack of high end sales at very small gross footages and (unexpectedly) lower rates of high end sales at very high square footages as well

# A plot showing high end vs. non high end sales by year built and block
ggplot(data=train_prepared)+geom_point(mapping =aes(x=YEAR.BUILT, 
                                                    y=BLOCK,color=high.end))
# Though there is no clear trend, it appears that the houses built most recently (after 2000) have a higher likelihood of being in high end sales

# A plot showing high end vs non high end sales within the midtown east neighborhood by land square footage and gross square footage
ggplot(data=train_prepared%>%filter(NEIGHBORHOOD=="MIDTOWN EAST"))+
  geom_point(mapping =aes(x=LAND.SQUARE.FEET,y=GROSS.SQUARE.FEET,color=high.end))
# This plot reveals that the vast majority of records have very low (1 ft^2) land square footages, which makes it hard to see any real trend, although of larger land properties with large (>1000 ft^2) gross square footages, a high proportion are high end sales.
```
Based on these plots it appears that building models to predict high end sales will be a pretty difficult process because there are no obvious direct correlations between these features and the high end outcome variable. 

## Logistic Regression
```{r}

#model 1
model <- glm(high.end~ NEIGHBORHOOD + BUILDING.CLASS.CATEGORY + 
             TAX.CLASS.AT.PRESENT + BLOCK + LOT + RESIDENTIAL.UNITS +
             COMMERCIAL.UNITS + LAND.SQUARE.FEET + GROSS.SQUARE.FEET +
             YEAR.BUILT + TAX.CLASS.AT.TIME.OF.SALE, family=binomial)
summary(model)
pred <- predict(model,newdata=train_prepared)

rates <- prediction(pred, train_prepared$high.end)
roc_result <- performance(rates,measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve") + lines(x = c(0,1), y = c(0,1), col="red")

confusion.mat <- table(train_prepared$high.end,pred > 0.5)
confusion.mat

(8174+2021)/(14150)


## model 2 using Logistic regression
model2<-glm(high.end~ NEIGHBORHOOD + BUILDING.CLASS.CATEGORY +
             BLOCK + LOT + BUILDING.CLASS.AT.PRESENT + RESIDENTIAL.UNITS +
             COMMERCIAL.UNITS + GROSS.SQUARE.FEET+ TAX.CLASS.AT.TIME.OF.SALE +
             BUILDING.CLASS.AT.TIME.OF.SALE, family=binomial)

pred2<-predict(model,newdata=train_prepared)

rates2<-prediction(pred2, train_prepared$high.end)
roc_result2<-performance(rates2,measure="tpr", x.measure="fpr")
plot(roc_result2, main="ROC Curve")+lines(x = c(0,1), y = c(0,1), col="red")

confusion.mat2<-table(train_prepared$high.end,pred2 > 0.5)
confusion.mat2
```
When we were choosing which features to use for our logistic regression, we inputted every feature that can be used. For model 2, we took away a few features that was indicated as insiginificant by the summary function. However, at the end, model 1 had a better training accuracy, so we used model 1 to predict the high.end for the test. However, this could be an indication of overfitting. 

## Support Vector Machine
```{r}
library(boot) ##for cv.glm function
library(MASS) ##for lda function
library(ipred) ##for errorest function
library(klaR) ##for partimat

# Linear Discriminant Analysis, with all quantitative features
lda.data <- lda(high.end~BLOCK+LOT+LAND.SQUARE.FEET+
                  YEAR.BUILT+GROSS.SQUARE.FEET
                ,train_prepared)
test_pred<-predict(lda.data,newdata=train_prepared)
table(train$high.end,test_pred$class)
mean(train$high.end==test_pred$class)
plot(lda.data, col = as.integer(train_prepared$high.end), main="LDA")

pred<-predict(lda.data,newdata=test_prepared) #73.7%
high.end<-pred$class

#### qda
# Quadratic Discriminant Analysis, with all quantitative features
qda.data <- qda(high.end~BLOCK+LOT+LAND.SQUARE.FEET+
                  YEAR.BUILT+GROSS.SQUARE.FEET
                ,train_prepared)
test_pred<-predict(qda.data,newdata=train_prepared)
table(train$high.end,test_pred$class)
mean(train$high.end==test_pred$class)

pred<-predict(lda.data,newdata=test_prepared) #75.321%
high.end<-pred$class
```
Support Vector Machine only take quantitative features, so we knew that the model prediction will be limited. 

##Random Forest
```{r}
# same features as the logistic regression
rnd.class<-randomForest(high.end ~ NEIGHBORHOOD + BUILDING.CLASS.CATEGORY +
                         TAX.CLASS.AT.PRESENT + BLOCK + LOT + RESIDENTIAL.UNITS 
                        + COMMERCIAL.UNITS + LAND.SQUARE.FEET +
                          TAX.CLASS.AT.TIME.OF.SALE +
                          GROSS.SQUARE.FEET,data=train_prepared,mtry=6,
                        importance=TRUE) 
rnd.class
importance(rnd.class) #15.11% error rate
#Neighborhood and Building Class Category are the most important

```



```{r}
# necessary to prevent leveling error from happening
levels(test_prepared$NEIGHBORHOOD) <- levels(train_prepared$NEIGHBORHOOD)
levels(test_prepared$BUILDING.CLASS.CATEGORY) <- levels(train_prepared$BUILDING.CLASS.CATEGORY)
levels(test_prepared$TAX.CLASS.AT.PRESENT) <- levels(train_prepared$TAX.CLASS.AT.PRESENT)
levels(test_prepared$BUILDING.CLASS.AT.PRESENT) <- levels(train_prepared$BUILDING.CLASS.AT.PRESENT)
levels(test_prepared$ADDRESS) <- levels(train_prepared$ADDRESS)
levels(test_prepared$BLOCK) <- levels(train_prepared$BLOCK)
levels(test_prepared$LOT) <- levels(train_prepared$LOT)
levels(test_prepared$RESIDENTIAL.UNITS) <- levels(train_prepared$RESIDENTIAL.UNITS)
levels(test_prepared$COMMERCIAL.UNITS) <- levels(train_prepared$COMMERCIAL.UNITS)
levels(test_prepared$GROSS.SQUARE.FEET) <- levels(train_prepared$GROSS.SQUARE.FEET)
levels(test_prepared$TAX.CLASS.AT.TIME.OF.SALE) <- levels(train_prepared$TAX.CLASS.AT.TIME.OF.SALE)
levels(test_prepared$LAND.SQUARE.FEET) <- levels(train_prepared$LAND.SQUARE.FEET)
levels(test_prepared$BUILDING.CLASS.AT.PRESENT) <- levels(train_prepared$BUILDING.CLASS.AT.PRESENT)
levels(test_prepared$ADDRESS) <- levels(train_prepared$ADDRESS)
levels(test_prepared$BUILDING.CLASS.AT.TIME.OF.SALE) <- levels(train_prepared$BUILDING.CLASS.AT.TIME.OF.SALE)
levels(test_prepared$SALE.DATE) <- levels(train_prepared$SALE.DATE)

rnd.pred<-predict(rnd.class,newdata=test_prepared) 
```
Surprisingly, the random forest model yielded poor performance on the test data with approximately 68% accuracy and 71% accuracy respectively for the two models. 


## Boosting 
```{r}
library(adabag)
boost.class<-boosting(high.end~NEIGHBORHOOD + BUILDING.CLASS.CATEGORY +
                   TAX.CLASS.AT.PRESENT + BLOCK + LOT+ LAND.SQUARE.FEET +
                   TAX.CLASS.AT.TIME.OF.SALE +GROSS.SQUARE.FEET, 
                 data=train_prepared,boos=F,mfinal=20)
#boost.class
pred.train.boost<-predict(boost.class, newdata=train_prepared, 
                          n.trees=500, type = "response")

mean(pred.train.boost$class==train_prepared$high.end) #0.8209894 accuracy
CrossTable(train_prepared$high.end, boost.class$class,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual diagnosis', 'predicted diagnosis'))


boost2 <- boosting(high.end ~ NEIGHBORHOOD+TAX.CLASS.AT.PRESENT+BLOCK+LOT+
                     RESIDENTIAL.UNITS+COMMERCIAL.UNITS+
                     TOTAL.UNITS+LAND.SQUARE.FEET+
                     GROSS.SQUARE.FEET+YEAR.BUILT+TAX.CLASS.AT.TIME.OF.SALE
                   , data=train_prepared, boos = FALSE, mfinal=50)
errorevol(boost2, train_prepared)

pred.train.boost2<-predict(boost2, newdata=train_prepared, 
                          n.trees=500, type = "response")

mean(pred.train.boost2$class==train_prepared$high.end) #0.8344876
CrossTable(train_prepared$high.end, boost2$class,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual diagnosis', 'predicted diagnosis'))
#Grid <- expand.grid(maxdepth=c(1,2,3,4,5,6,7),nu=.01,iter=c(50,100,150,200))
#results_ada = train(high.end~., data=train_prepared, method="ada", trControl=cv_opts,tuneGrid=Grid)
```
The boost also massively overfit the data; for the training set, it yielded 71.1% accuracy.

## Dense Neural Network
The final machine learning method applied to this problem was a Dense Neural Network (DNN). Due to the computational complexity and wide variety of hyperparameters that must be tuned, this analysis was done in a seperate .ipynb file attached to this project. The goal of the DNN is the same as the methods above: binary classification. In order to accomplish this, the final layer of the network must be a single node that's activation function is the sigmoid function. The other hidden layers of the DNN use the industry standard relu function for activation of nodes. The architecture of the DNN is that of a funnel. Due to the large number of input dimensions, a result of one hot encoding categorical variables with many dimensions, through many levels of testing it was decided that this was the best was to coerce the DNN into sufficiently condensing all the information the training data had to offer. At each hidden layer a kernel constraint was added to ensure that no single node could "take over" the entire network and skew predictions. Again through a series of testing, it was decided that the adam optimizer and binary cross-entropy loss function should be used. Adam is a well-known optimizer of deep neural networks and allows the gradient descent function to "remember" recent gradients so that any one too aggressive step is not taken as local/global maxima are converged upon. Binary cross-entropy is the standard loss function suggested for binary classification problems due to it's binary nature. Multi-level classification problems use a slightly different loss function for any number of response classes greater than two. 

## Model Aggregation
After finalizing models from each type of algorithm explained above, we had each predict the response variable for the test set. A majority vote approach was used to compile a final prediction for each instance in the test set. This is similar to the idea behind the bagging and random forest methods, averaging the predictions of multiple models. Under the statistical tenet that averaging reduces variance, combining the predictions of the above models should allow us to better predict the response variable based on the unseen test data. As well, since each of these models were made in vastly different ways, they are likely far less correlated than each decision tree from the bagging method, for example. Utilizing another statistical tenet, that averaging uncorrelated predictions results in an even lower variance, this should allow us to boost the accuracy of our final submitted predictions. 
