---
title: "Project 2 Final"
author: "Siyeon Shaun Kim (sk5ps), Alex Stern (acs4wq), Josh Eiland (jhe5ah)"
date: "11/26/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(lubridate)
```

## Data Cleaning and Feature Engineering
```{r}
# importing the train and test data sets
train <- read.csv("sales_manhattan_train_set.csv")
test <- read.csv("sales_manhattan_test_set.csv")
# we must coerce the data into more purposeful data types
# high end, tax class, and zip code should be categorical
train$high.end <- as.factor(train$high.end)
train$TAX.CLASS.AT.TIME.OF.SALE <- as.factor(train$TAX.CLASS.AT.TIME.OF.SALE)
train$ZIP.CODE <- as.factor(train$ZIP.CODE)
# and square footages should be numeric
train$LAND.SQUARE.FEET <- as.numeric(train$LAND.SQUARE.FEET)
train$GROSS.SQUARE.FEET <- as.numeric(train$GROSS.SQUARE.FEET)
# missing data in YEAR.BUILT must be accounted for
# binary T/F variables are created if the building had any commerical/residential units
levels(train$BUILDING.CLASS.AT.TIME.OF.SALE) <- levels(train$BUILDING.CLASS.AT.PRESENT)
train <- train %>% mutate(
  COMMERCIAL.UNITS = ifelse(COMMERCIAL.UNITS==0,F,T),
  RESIDENTIAL.UNITS = ifelse(RESIDENTIAL.UNITS==0,F,T),
  TOTAL.UNITS = ifelse(TOTAL.UNITS==0,F,T),
  # assuming that buildings missing years were constructed in 1932
  # since this is when our records begin
  YEAR.BUILT=ifelse(YEAR.BUILT==0,1932,YEAR.BUILT)
)
# We also attempted to look into whether the building class had changed and whether this was a relevant factor using the following code:
# BUILDING.CLASS.CHANGE = ifelse(BUILDING.CLASS.AT.PRESENT!=BUILDING.CLASS.AT.TIME.OF.SALE,T,F),
# but this revealed that in fact the building class had changed for every single record in the dataset, meaning that it wasn't a helpful engineered feature

# Lubridate was used to extract the month, day, and year of the sale
train$sale_year <- year(mdy(train$SALE.DATE))
train$sale_month <- as.factor(month(mdy(train$SALE.DATE)))
train$sale_day <- as.factor(day(mdy(train$SALE.DATE)))
train$sale_dayofweek <- as.factor(wday(mdy(train$SALE.DATE)))

# determining old the building is at the time of sale?
train$sale_minus_built <- (train$sale_year - train$YEAR.BUILT)
# stripping the house numbers and apartment numbers from addresses to get just the street names
train$ADDRESS <- as.factor(gsub(",.*$", "", sub("^\\S+\\s+", '', train$ADDRESS)))
# drop features that will not be used
train_prepared <- train %>% dplyr::select(
  -BOROUGH, -EASE.MENT, -APARTMENT.NUMBER
)

summary(train_prepared)
```
We got rid of BOROUGH, EASE.MENT, and APARTMENT.NUMBER because the BOROUGH variable has no variation in values (always 1) and all the EASE.MENT values are NA, while the apartment number (though potentially useful with enough advanced parcing) was too complex and variable for us to break down. We stripped the details of the addresses to include just the street names so that it could potentially be used to help predict values of future sales on the same street. We also factorized high.end, TAX.CLASS.AT.TIME.OF.SALE, ZIP.CODE, RESIDENTIAL UNITS, COMMERCIAL UNITS, sale_month, sale_day, and sale_day_of_week. Below, we implemented the same data process to the test training set.

```{r}
# we must coerce the data into more purposeful data types
# high end, tax class, and building classes should be categorical
test$TAX.CLASS.AT.TIME.OF.SALE <- as.factor(test$TAX.CLASS.AT.TIME.OF.SALE)
test$ZIP.CODE <- as.factor(test$ZIP.CODE)
# and square footages should be numeric
test$LAND.SQUARE.FEET <- as.numeric(test$LAND.SQUARE.FEET)
test$GROSS.SQUARE.FEET <- as.numeric(test$GROSS.SQUARE.FEET)
# missing data in YEAR.BUILT must be accounted for
# binary T/F variables are created if the building had any commerical/residential units
test <- test %>% mutate(
  COMMERCIAL.UNITS=ifelse(COMMERCIAL.UNITS==0,F,T),
  RESIDENTIAL.UNITS=ifelse(RESIDENTIAL.UNITS==0,F,T),
  TOTAL.UNITS=ifelse(TOTAL.UNITS==0,F,T),
  # assuming that buildings missing years were constructed in 1932
  # since this is when our records begin
  YEAR.BUILT=ifelse(YEAR.BUILT==0,1932,YEAR.BUILT)
)
# lubridate used to extract the month, day, and year of the sale
test$sale_year <- year(mdy(test$SALE.DATE))
test$sale_month <- as.factor(month(mdy(test$SALE.DATE)))
test$sale_day <- as.factor(day(mdy(test$SALE.DATE)))
test$sale_dayofweek <- as.factor(wday(mdy(test$SALE.DATE)))
# how old is the building at the time of sale?
test$sale_minus_built <- (test$sale_year - test$YEAR.BUILT)
# stripping the house numbers and apartment numbers from addresses to get just the street names
test$ADDRESS <- as.factor(gsub(",.*$", "", sub("^\\S+\\s+", '', test$ADDRESS)))
# drop features that will not be used
test_prepared <- test %>% dplyr::select(
  -BOROUGH, -EASE.MENT, -APARTMENT.NUMBER
)

summary(test_prepared)
```

## Exploratory Data Analysis 
```{r}
# A plot showing high end vs. non high end sales by block and gross square footage
ggplot(data=train_prepared)+geom_point(mapping =aes(x=BLOCK, 
                                                       y=GROSS.SQUARE.FEET,color=high.end))
# It reveals little clear correlation other than an apparent lack of high end sales at very small gross footages and (unexpectedly) lower rates of high end sales at very high square footages as well

# A plot showing high end vs. non high end sales by year built and block
ggplot(data=train_prepared)+geom_point(mapping =aes(x=YEAR.BUILT, 
                                                    y=BLOCK,color=high.end))
# Though there is no clear trend, it appears that the houses built most recently (after 2000) have a higher likelihood of being in high end sales

# A plot showing high end vs non high end sales within the midtown east neighborhood by land square footage and gross square footage
ggplot(data=train_prepared%>%filter(NEIGHBORHOOD=="MIDTOWN EAST"))+
  geom_point(mapping =aes(x=LAND.SQUARE.FEET,y=GROSS.SQUARE.FEET,color=high.end))
# This plot reveals that the vast majority of records have very low (1 ft^2) land square footages, which makes it hard to see any real trend, although of larger land properties with large (>1000 ft^2) gross square footages, a high proportion are high end sales.
```
Based on these plots it appears that building models to predict high end sales will be a pretty difficult process because there are no obvious direct correlations between these features and the high end outcome variable. 

## Logistic Regression
```{r}
library(glmnet)
library(ROCR)

summary(train_prepared)
summary(test_prepared)
attach(train_prepared)

model <- glm(high.end~ NEIGHBORHOOD + BUILDING.CLASS.CATEGORY + 
             TAX.CLASS.AT.PRESENT + BLOCK + LOT + RESIDENTIAL.UNITS +
             COMMERCIAL.UNITS + LAND.SQUARE.FEET + GROSS.SQUARE.FEET +
             YEAR.BUILT + TAX.CLASS.AT.TIME.OF.SALE, family=binomial)
summary(model)
pred <- predict(model,newdata=train_prepared)

rates <- prediction(pred, train_prepared$high.end)
roc_result <- performance(rates,measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve") + lines(x = c(0,1), y = c(0,1), col="red")

confusion.mat <- table(train_prepared$high.end,pred > 0.5)
confusion.mat

(8174+2021)/(14150)


## model 2 using Logistic regression
model2<-glm(high.end~ NEIGHBORHOOD + BUILDING.CLASS.CATEGORY +
             BLOCK + LOT + BUILDING.CLASS.AT.PRESENT + RESIDENTIAL.UNITS +
             COMMERCIAL.UNITS + GROSS.SQUARE.FEET+ TAX.CLASS.AT.TIME.OF.SALE +
             BUILDING.CLASS.AT.TIME.OF.SALE, family=binomial)

pred2<-predict(model,newdata=train_prepared)

rates2<-prediction(pred2, train_prepared$high.end)
roc_result2<-performance(rates2,measure="tpr", x.measure="fpr")
plot(roc_result2, main="ROC Curve")+lines(x = c(0,1), y = c(0,1), col="red")

confusion.mat2<-table(train_prepared$high.end,pred2 > 0.5)
confusion.mat2
```

## Support Vector Machine
```{r}
source("datacleaning.r")
library(boot) ##for cv.glm function
library(MASS) ##for lda function
library(ipred) ##for errorest function
library(klaR) ##for partimat

lda.data <- lda(high.end~BLOCK+LOT+LAND.SQUARE.FEET+
                  YEAR.BUILT+GROSS.SQUARE.FEET
                ,train_prepared)
test_pred<-predict(lda.data,newdata=train_prepared)
table(train$high.end,test_pred$class)
mean(train$high.end==test_pred$class)
plot(lda.data, col = as.integer(train_prepared$high.end), main="LDA")

pred<-predict(lda.data,newdata=test_prepared)
high.end<-pred$class

#### qda

qda.data <- qda(high.end~BLOCK+LOT+LAND.SQUARE.FEET+
                  YEAR.BUILT+GROSS.SQUARE.FEET
                ,train_prepared)
test_pred<-predict(qda.data,newdata=train_prepared)
table(train$high.end,test_pred$class)
mean(train$high.end==test_pred$class)

pred<-predict(lda.data,newdata=test_prepared)
high.end<-pred$class
```
Support Vector Machine only takes quantitative features, so we knew that the model prediction will be limited. 

##Random Forest
```{r}
library(tree)
library(randomForest) ##for random forests (and bagging)
library(gbm) ##for boosting

rnd.class<-randomForest(high.end ~ NEIGHBORHOOD + BUILDING.CLASS.CATEGORY +
                         TAX.CLASS.AT.PRESENT + BLOCK + LOT + RESIDENTIAL.UNITS 
                        + COMMERCIAL.UNITS + LAND.SQUARE.FEET +
                          TAX.CLASS.AT.TIME.OF.SALE +
                          GROSS.SQUARE.FEET,data=train_prepared,mtry=6,
                        importance=TRUE) 
rnd.class
importance(rnd.class)

# Built a second random forest model using all variables except those with more than 53 factors (building class at present and time of sale, address, and sale date)
rf2 <- randomForest(train_prepared[,-c(7:9,18:20)],train_prepared$high.end,
                     sampsize=round(0.6*length(train_prepared[,20])),
                     ntree=500,mtry=sqrt(16),importance=TRUE)
rf2

levels(test_prepared$NEIGHBORHOOD) <- levels(train_prepared$NEIGHBORHOOD)
levels(test_prepared$BUILDING.CLASS.CATEGORY) <- levels(train_prepared$BUILDING.CLASS.CATEGORY)
levels(test_prepared$TAX.CLASS.AT.PRESENT) <- levels(train_prepared$TAX.CLASS.AT.PRESENT)
levels(test_prepared$BUILDING.CLASS.AT.PRESENT) <- levels(train_prepared$BUILDING.CLASS.AT.PRESENT)
levels(test_prepared$ADDRESS) <- levels(train_prepared$ADDRESS)
levels(test_prepared$BLOCK) <- levels(train_prepared$BLOCK)
levels(test_prepared$LOT) <- levels(train_prepared$LOT)
levels(test_prepared$RESIDENTIAL.UNITS) <- levels(train_prepared$RESIDENTIAL.UNITS)
levels(test_prepared$COMMERCIAL.UNITS) <- levels(train_prepared$COMMERCIAL.UNITS)
levels(test_prepared$GROSS.SQUARE.FEET) <- levels(train_prepared$GROSS.SQUARE.FEET)
levels(test_prepared$TAX.CLASS.AT.TIME.OF.SALE) <- levels(train_prepared$TAX.CLASS.AT.TIME.OF.SALE)
levels(test_prepared$LAND.SQUARE.FEET) <- levels(train_prepared$LAND.SQUARE.FEET)


rnd.pred<-predict(rnd.class,newdata=test_prepared) 

rfpred <- predict(rf2,newdata=test_prepared)
```
## Dense Neural Network
The final machine learning method applied to this problem was a Dense Neural Network (DNN). Due to the computational complexity and wide variety of hyperparameters that must be tuned, this analysis was done in a seperate .ipynb file attached to this project. The goal of the DNN is the same as the methods above: binary classification. In order to accomplish this, the final layer of the network must be a single node that's activation function is the sigmoid function. The other hidden layers of the DNN use the industry standard relu function for activation of nodes. The architecture of the DNN is that of a funnel. Due to the large number of input dimensions, a result of one hot encoding categorical variables with many dimensions, through many levels of testing it was decided that this was the best was to coerce the DNN into sufficiently condensing all the information the training data had to offer. At each hidden layer a kernel constraint was added to ensure that no single node could "take over" the entire network and skew predictions. Again through a series of testing, it was decided that the adam optimizer and binary cross-entropy loss function should be used. Adam is a well-known optimizer of deep neural networks and allows the gradient descent function to "remember" recent gradients so that any one too aggressive step is not taken as local/global maxima are converged upon. Binary cross-entropy is the standard loss function suggested for binary classification problems due to it's binary nature. Multi-level classification problems use a slightly different loss function for any number of response classes greater than two. 
